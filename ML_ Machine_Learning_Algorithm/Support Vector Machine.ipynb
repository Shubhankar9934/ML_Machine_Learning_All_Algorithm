{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9386b7",
   "metadata": {},
   "source": [
    "# What is Support Vector Machines?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9b1d987",
   "metadata": {},
   "source": [
    "The SVM model is a supervised machine learning model that is mainly used\n",
    "for classifications (but it could also be used for regression!). It learns\n",
    "how to separate different groups by forming decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22d54b",
   "metadata": {},
   "source": [
    "# Kernel Tricks ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5278ea6",
   "metadata": {},
   "source": [
    "When talking about kernels in machine learning , most likely the first thing \n",
    "that comes into your mind is support vector machines (SVM) model because the\n",
    "kernal trick is widely used in the SVM Model to bridge linearity and non-Linearity.\n",
    "\n",
    " The “trick” is that kernel methods represent the data only through a set of pairwise similarity comparisons between the original data observations x (with the original coordinates in the lower dimensional space), instead of explicitly applying the transformations ϕ(x) and representing the data by these transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad5bb2",
   "metadata": {},
   "source": [
    "# Why is it important to use the kernel trick?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89c300ea",
   "metadata": {},
   "source": [
    "It allows us to operate in the original feature space without computing the \n",
    "coordinates of the data in a higher dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193e238",
   "metadata": {},
   "source": [
    "# Briefly speaking about kernal!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bd4f197",
   "metadata": {},
   "source": [
    "Briefly speaking, a kernel is a shortcut that helps us do certain calculation\n",
    "faster which otherwise would involve computations in higher dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e2ecd",
   "metadata": {},
   "source": [
    "# Mathematical definition"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c580ed4",
   "metadata": {},
   "source": [
    "Mathematical definition: K(x, y) = <f(x), f(y)>. Here K is the kernel function,\n",
    "x, y are n dimensional inputs. f is a map from n-dimension to m-dimension space.\n",
    "< x,y> denotes the dot product. usually m is much larger than n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305f31b",
   "metadata": {},
   "source": [
    "# Intuition: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b284e9e5",
   "metadata": {},
   "source": [
    "Intuition: normally calculating <f(x), f(y)> requires us to calculate f(x),\n",
    "f(y) first, and then do the dot product. These two computation steps can be\n",
    "quite expensive as they involve manipulations in m dimensional space,\n",
    "where m can be a large number. But after all the trouble of going to the high\n",
    "dimensional space, the result of the dot product is really a scalar: \n",
    "we come back to one-dimensional space again! Now, the question we have is:\n",
    "do we really need to go through all the trouble to get this one number? \n",
    "do we really have to go to the m-dimensional space? The answer is no, if\n",
    "you find a clever kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2676c",
   "metadata": {},
   "source": [
    "# Simple Example:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd224fd3",
   "metadata": {},
   "source": [
    "Simple Example: x = (x1, x2, x3); y = (y1, y2, y3). \n",
    "Then for the function f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1, x3x2, x3x3), \n",
    "the kernel is K(x, y ) = (<x, y>)^2.\n",
    "\n",
    "Let's plug in some numbers to make this more intuitive: suppose x = (1, 2, 3); y = (4, 5, 6). Then:\n",
    "f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)\n",
    "f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)\n",
    "<f(x), f(y)> = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024\n",
    "\n",
    "A lot of algebra. Mainly because f is a mapping from 3-dimensional to 9 dimensional space.\n",
    "\n",
    "Now let us use the kernel instead:\n",
    "K(x, y) = (4 + 10 + 18 ) ^2 = 32^2 = 1024\n",
    "Same result, but this calculation is so much easier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa95c15",
   "metadata": {},
   "source": [
    "# Additional beauty of Kernel:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff207c4b",
   "metadata": {},
   "source": [
    "Additional beauty of Kernel: kernels allow us to do stuff in infinite \n",
    "dimensions! Sometimes going to higher dimension is not just computationally \n",
    "expensive, but also impossible. f(x) can be a mapping from n dimension to\n",
    "infinite dimension which we may have little idea of how to deal with.\n",
    "Then kernel gives us a wonderful shortcut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf531ee",
   "metadata": {},
   "source": [
    "# Relation to SVM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "307c7eae",
   "metadata": {},
   "source": [
    "Relation to SVM: now how is related to SVM? The idea of SVM is that\n",
    "y = w phi(x) +b, where w is the weight, phi is the feature vector,\n",
    "and b is the bias. if y> 0, then we classify datum to class 1, else to class 0.\n",
    "We want to find a set of weight and bias such that the margin is maximized.\n",
    "Previous answers mention that kernel makes data linearly separable for SVM.\n",
    "I think a more precise way to put this is, kernels do not make the data \n",
    "linearly separable. The feature vector phi(x) makes the data linearly separable.\n",
    "Kernel is to make the calculation process faster and easier, especially when \n",
    "the feature vector phi is of very high dimension \n",
    "(for example, x1, x2, x3, ..., x_D^n, x1^2, x2^2, ...., x_D^2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "269871c9",
   "metadata": {},
   "source": [
    "Why it can also be understood as a measure of similarity:\n",
    "if we put the definition of kernel above, <f(x), f(y)>, in the context of\n",
    "SVM and feature vectors, it becomes <phi(x), phi(y)>. The inner product means\n",
    "the projection of phi(x) onto phi(y). or colloquially, how much overlap do x\n",
    "and y have in their feature space. In other words, how similar they are."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
